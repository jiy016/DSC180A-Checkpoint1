# DSC180A-Checkpoint1


# What is this project about: Weak-to-Strong Generalization on the MATH Dataset

## 1. Problem & Goal
We are testing whether a stronger model can recover performance using few-shot examples generated by a weaker model. Using weak supervision with weak model will or will not improve the performance.

## 2. Setup (at this point, adding more assessment and selection later)
- **Weak model:** gpt-4.1-nano  
- **Strong model:** gpt-4o-mini  
- **Dataset:** MATH (200 train, 200 test)
- **Metric:** Performance Gap Recovered (PGR) value
- **k:** 4 or 8 examples

## 3. Method
1. Use weak model to generate answers for `k` train questions.
2. Build two few-shot prompts:
   - GoldFS: uses true answers
   - WeakFS: uses weak model’s generated answers
3. Evaluate accuracy on test set under three settings:
   - Weak + GoldFS  
   - Strong + WeakFS  
   - Strong + GoldFS  

## 4. Results
| k | Weak+GoldFS | Strong+WeakFS | Strong+GoldFS | PGR |
|---|--------------|---------------|----------------|-----|
| 4 | 0.47 | 0.43 | 0.57 | -0.33 |
| 8 | 0.38 | 0.10 | 0.23 | 1.87 |

## 5. Analysis
- Negative or >1 PGR indicates instability due to small sample size or parsing issues.
- GoldFS upper bound sometimes underperforms weak baseline, suggesting format sensitivity.
- Increasing test size (n=100) and enforcing `<answer>` consistency improves stability.

## 6. Insights
- Weak exemplars help only when their reasoning format matches gold structure.
- Strong model is sensitive to noisy weak examples.
- Future work: sample filtering, weighted exemplars, and prompt instruction tuning.

## 7. Cost Control
- Batched 10-query requests, reused cached few-shot examples.
- Each run under 30 test items ≈ \$0.05 API cost.

## 8. Next Steps
Before the next checkpoint, I plan to improve by:
1. **Increase test coverage** to 100+ problems for statistical reliability.  
2. **Filter or weight weak examples** by confidence or correctness.  
3. **Visualize trends** across `k` and model gaps for a more complete picture.

